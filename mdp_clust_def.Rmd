---
title: "mdp_clust"
author: "Áprado"
date: "2024-04-26"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,message=FALSE,warning = FALSE)

```

## LIBRERÍAS

Ahora vamos a cargar todas las librerías necesatia para poder desarrollar el trabajo adecuadamente

```{r}
library(readxl)
library(knitr)
library(dplyr)
library(FactoMineR)
library(factoextra)
library(grid)
library(cluster)
library(gridExtra)
library(mice)
library(reshape2)
library(writexl)
library(ggplot2)
```

## Carga de datos

Después de la carga de librerías, pasaremos a realizar la carga de la base de datos que vamos a utilizar durante esta parte del trabajo.

```{r}
daly=read_excel("Base_de_datos.xlsx")
nombres_sin_comillas <- c("Pais", "Codigo", "Año", "Autolesion", "F_Naturaleza", "Conflictos", "Vio_IntPer", "Enf_Tropicales", "Consumo_sustancias", "Enf_cutanea", "Inf_Entericas", "Diabetes", "Enf_cardiovasculares", "Enf_digestivas", "Def_Nutricion", "Inf_Respiratorias", "Des_Neonatales", "Enf_Resp_Cronicas", "Otras_Enf", "Des_Maternos", "Les_NoInt", "Des_MuscEsc", "Neoplasmas", "Des_mentales", "Des_Neuro", "ETS", "Les_Transporte", "Enf_OrgSens")
colnames(daly)=nombres_sin_comillas
```

Eliminamos la observación 4505, perteneciente al año en 1994 en Ruanda. Debido a que se produjo un gran genocidio y esto condiciona el análisis debido a que este dato no sigue el canon normal, y por tanto nos desvirtúa los resultados.

```{r}
daly <- daly[-4505, ]
```

Ahora creamos un dataframe que nos permita describir cada una de las variables, es decir, crearemos dos columnas en las que en la primera estará el nombre de la variable y una segunda donde pondremos el tipo de variable que vamos a analizar, esto es, numérica, texto...

```{r}
desc_daly = data.frame("variable" = colnames(daly),
                       "tipo" = c("text", "text", "integer", rep("numerical", 25)), stringsAsFactors = FALSE)
```

Logaritmizamos para reducir la variabilidad de los datos ya que tenemos países con valores muy pequeños de las variables frente a otros donde los índices son muy grandes (paises en guerra). Por otra parte, centramos los datos para facilitar su comprension, pero no escalamos porque todas las variables tienen las mismas unidades. Seleccionamos las variables que son de tipo numérico, ya que las de tipo texto no nos sirven para hacer el clustering y las dejamos a parte para este análisis

```{r}
cent=daly[4:28] #Seleccionamos las variables de tipo numérico, sabemos que son todas estas, por eso la selección se realiza desde aquí
cent = cent%>%  mutate_if(~ min(., na.rm = TRUE) == 0, ~ . + 1)
log <- cent %>%
  mutate_if(is.numeric, list(~ log(.)))
log=scale(log, center=TRUE, scale=FALSE)
log=as.data.frame(log); 
log_daly = cbind(daly[1:3], log)
log_daly=log_daly[, desc_daly$tipo=="numerical"]

```

## Clustering

Comenzamos ahora el clustering: En primer lugar, calcularemos la matriz de distancias para nuestras observaciones, en este caso lo realizaremos con la medida de distancia euclídea:

```{r}
midist <- get_dist(log_daly, stand = FALSE, method = "euclidean")
#fviz_dist(midist, show_labels = TRUE, lab_size = 0.3,
 #         gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))

```

Para decidir en cuántos clusters vamos a dividir nuestras observaciones vamos a utilizar diferentes métodos jerárquicos y de partición. Todos ellos basados en la distancia euclídea.

Elegimos esta medida de distancia porque para nuestro proyecto es más intersante agrupar observaciones con valores similares de DALY, es decir con una incidencia de las causas de mortalidad parecida, en vez de países que sigan la misma variación entre causas.

### Modelos jerárquicos

#### Método de WARD

##### Gráfico de silhouette y de suma de cuadrados intra-cluster

```{r}
p1 = fviz_nbclust(x = log_daly, FUNcluster = hcut, method = "silhouette", 
                  hc_method = "ward.D2", k.max = 10, verbose = FALSE, 
                  hc_metric = "euclidean") + labs(title = "Num. optimo clusters")
p2 = fviz_nbclust(x = log_daly, FUNcluster = hcut, method = "wss", 
                  hc_method = "ward.D2", k.max = 10, verbose = FALSE, 
                  hc_metric = "euclidean") + labs(title = "Num. optimo clusters")
grid.arrange(p1, p2, nrow = 1)
```

Se aprecia que el máximo en el estadístico de Silhoutte se obtiene con dos clusters. No obstante, la suma de cuadrados es demasiado elevada para esta cantidad de clusters, por lo que decidimos pasar al siguiente número, con 3 clusters el estadístico de silhoutte es menor pero desciende la suma de cuadrados intra-cluster, teniendo un valor suficientemente pequeño, y al observar las tendencias posteriores, no hay tanta diferencia, por lo que este también podría ser una opción.

Ahora describiremos , una pequeña tabla con 2 y 3 clusters para este método, para ver como se distribuyen las observaciones y para posteriormente realizar los gráficos, con cada uno de los tipos.

```{r}
clust1 <- hclust(midist, method="ward.D2")
grupos1 <- cutree(clust1, k=2)
table(grupos1)

grupos1b <- cutree(clust1, k=3)
table(grupos1)
```

En este gráfico podemos observar 2 clusters:

```{r}
fviz_cluster(object = list(data=log_daly, cluster=grupos1), stand = FALSE,
             ellipse.type = "convex", geom = "point", show.clust.cent = FALSE,
             labelsize = 8)  +
  labs(title = "Modelo jerarquico + Proyeccion PCA",
       subtitle = "Dist euclidea, Metodo Ward, K=2") +
  theme_bw() +
  theme(legend.position = "bottom")
```

En este gráfico podemos observar 3 clusters

```{r}
fviz_cluster(object = list(data=log_daly, cluster=grupos1b), stand = FALSE,
             ellipse.type = "convex", geom = "point", show.clust.cent = FALSE,
             labelsize = 8)  +
  labs(title = "Modelo jerarquico + Proyeccion PCA",
       subtitle = "Dist euclidea, Metodo Ward, K=3") +
  theme_bw() +
  theme(legend.position = "bottom")
```

**Conclusiones:**

Tras observar detenidamente ambos gráficos, observamos como se producen solapamientos en ambos, es decir, que no se separan adecuadamente con la primera y segunda dimensión, esto se puede deber a que representamos 30 años seguidos de muchos países, y puede ser que estos tengan distancias sean relativamente parecidas y se acaben solapando, también puede ocurrir por las proximidades en los paises donde se pueden producir las mismas catástrofes, guerras, enfermedades. De momento consideramos este método como secundario, ya que queremos obtener uno que consiga una mejor diferenciación entre ellos.

#### Método de la Media

##### Gráfico de silhouette y de suma de cuadrados intra-cluster

```{r}
p1 = fviz_nbclust(x = log_daly, FUNcluster = hcut, method = "silhouette", 
                  hc_method = "average", k.max = 10, verbose = FALSE, 
                  hc_metric = "euclidean") + labs(title = "Num. optimo clusters")
p2 = fviz_nbclust(x = log_daly, FUNcluster = hcut, method = "wss", 
                  hc_method = "average", k.max = 10, verbose = FALSE, 
                  hc_metric = "euclidean") + labs(title = "Num. optimo clusters")
grid.arrange(p1, p2, nrow = 1)
```

Según el método de la media apreciamos que a partir de 4 clusters el total de la suma de cuadrados intra-cluster es prácticamente idéntica. Por otro lado, en lo que respecta al estadístico silhoutte cuando hay dos clusters se llega a un óptimo. Viendo los dos gráficos, podemos pensar que las dos opciones óptimas son 2 o 4 clusters. Con 2 clusters la silhoutte explicada es mayor, pero la suma de cuadrados intra-cluster es demasiado elevada. Con 4 clusters ocurre lo contrario, con una silhoutte media ligeramente menor pero con menos suma de cuadrados.

Ahora describiremos , una pequeña tabla con 2 y 3 clusters para este método, para ver como se distribuyen las observaciones y para posteriormente realizar los gráficos, con cada uno de los tipos.

```{r}
clust2 <- hclust(midist, method="average")
grupos2 = cutree(clust2, k = 2)
table(grupos2)

grupos2b = cutree(clust2, k = 4)
table(grupos2b)
```

Según el método de la media apreciamos que a partir de 4 clusters el total de la suma de cuadrados intra-cluster es prácticamente idéntica. Por otro lado, en lo que respecta al estadístico silhoutte cuando hay dos clusters se llega a un óptimo. Viendo los dos gráficos, podemos pensar que las dos opciones óptimas son 2 o 4 clusters. Con 2 clusters la silhoutte explicada es mayor, pero la suma de cuadrados intra-cluster es demasiado elevada. Con 4 clusters ocurre lo contrario, con una silhoutte media ligeramente menor pero con menos suma de cuadrados.

En este gráfico podemos observar 2 clusters:

```{r}
fviz_cluster(object = list(data=log_daly, cluster=grupos2), stand = FALSE,
             ellipse.type = "convex", geom = "point", show.clust.cent = FALSE,
             labelsize = 8)  +
  labs(title = "Modelo jerarquico + Proyeccion PCA",
       subtitle = "Dist euclidea, Metodo de la media, K=2") +
  theme_bw() +
  theme(legend.position = "bottom")
```

En este gráfico podemos observar 4 clusters:

```{r}
fviz_cluster(object = list(data=log_daly, cluster=grupos2b), stand = FALSE,
             ellipse.type = "convex", geom = "point", show.clust.cent = FALSE,
             labelsize = 8)  +
  labs(title = "Modelo jerarquico + Proyeccion PCA",
       subtitle = "Dist euclidea, Metodo de la media, K=4") +
  theme_bw() +
  theme(legend.position = "bottom")
```

**Conclusiones:**

Al igual que ocurre en los gráficos obtenidos con Ward, se obtiene sobreposición también para 2 y 4 clusters, sin embargo en esta es menor que en la anterior. Tras observar ambos gráficos, consideramos que es más adecuado escoger 2 clusters en vez de 4, puesto que el solapamiento entre ellos existe pero es menor para nuestro gusto. Anteriormente ya hemos explicado la posibildad de solapamiento a que se debe.

### Modelos de partición

#### K-MEDIAS

##### Gráfico de silhouette y de suma de cuadrados intra-cluster

```{r}
p1 = fviz_nbclust(x = log_daly, FUNcluster = kmeans, method = "silhouette", 
             k.max = 10, verbose = FALSE) +
  labs(title = "K-means")
p2 = fviz_nbclust(x = log_daly, FUNcluster = kmeans, method = "wss", 
             k.max = 10, verbose = FALSE) +
  labs(title = "K-means")
grid.arrange(p1, p2, nrow = 1)
```

Tras observar Silhouette, vemos que el valor medio más alto se obtiene con 2 clusters, pero tras observar la suma de cuadrados intra-cluster vemos que este presenta un valor todavía muy alto, por lo que consideramos adecuado descartar esta cantidad. Decidimos escoger 3, puesto que aunque tenga un valor de Silhouette menor, es bastante similar al valor máximo, y además su suma de cuadrados intra-cluster es muy inferior y más adecuada.

Ahora describiremos , una pequeña tabla con 3 clusters para este método, para ver como se distribuyen las observaciones y para posteriormente realizar los gráficos de scores.

```{r}
set.seed(100)
clust3 <- kmeans(log_daly, centers = 3, nstart = 20)
table(clust3$cluster)
```

En este gráfico podemos observar 3 clusters:

```{r}
fviz_cluster(object = list(data=log_daly, cluster=clust3$cluster), stand = FALSE,
             ellipse.type = "convex", geom = "point", show.clust.cent = FALSE,
             labelsize = 8)  +
  labs(title = "Modelo de partición + Proyeccion PCA",
       subtitle = "Dist euclidea, Metodo k-Medias, K=3") +
  theme_bw() +
  theme(legend.position = "bottom")
```

Tras analizar el gráfico, vemos que el solapamiento es mucho menor en comparación con los métodos anteriores y, por tanto, la explicación de los clusters es mejor, puesto que podemos diferenciarlos claramente. En esta visualización, se han identificado tres clusters utilizando el método de k-medias (k-means) con K=3, lo que ha permitido una mejor separación de los datos en el espacio de las componentes principales. Los puntos verdes (cluster 1), los triángulos azules (cluster 2) y los puntos rojos (cluster 3) muestran una mayor separación entre los clusters, lo que indica que este método proporciona una agrupación más clara y precisa. Además, la primera componente principal (Dim1) sigue siendo la que más contribuye a la variabilidad de los datos, explicando el 68.8% de la misma, mientras que la segunda componente principal (Dim2) explica un 8.8%.

#### K-MEDOIDES

##### Gráfico de silhouette y de suma de cuadrados intra-cluster

```{r}
p1 = fviz_nbclust(x = log_daly, FUNcluster = pam, method = "silhouette", 
             k.max = 10, verbose = FALSE) +
  labs(title = "K-mediods")
p2 = fviz_nbclust(x = log_daly, FUNcluster = pam, method = "wss", 
             k.max = 10, verbose = FALSE) +
  labs(title = "K-mediods")
grid.arrange(p1, p2, nrow = 1)
```

Tras observar el valor del índice de Silhouette, notamos que el valor medio más alto se obtiene con 2 clusters. Sin embargo, la suma de cuadrados intra-cluster sigue siendo muy alta en este caso, lo que nos lleva a descartar esta opción. Decidimos optar por 3 clusters, ya que, aunque el valor del índice de Silhouette es ligeramente menor que el máximo, sigue siendo bastante similar. Además, la suma de cuadrados intra-cluster es significativamente inferior y más adecuada. Aunque se podría considerar usar 4 clusters, hemos decidido no hacerlo debido a una considerable disminución en el valor medio del índice de Silhouette, lo que indica una peor calidad de los clusters formados.

Ahora describiremos , una pequeña tabla con 3 clusters para este método, para ver como se distribuyen las observaciones y para posteriormente realizar los gráficos de scores.

```{r}
clust4 <- pam(log_daly, k = 3)
table(clust4$clustering)
```

```{r}
fviz_cluster(object = list(data=log_daly, cluster=clust4$clustering), stand = FALSE,
             ellipse.type = "convex", geom = "point", show.clust.cent = FALSE,
             labelsize = 8)  +
  labs(title = "Modelo jerarquico + Proyeccion PCA",
       subtitle = "Dist euclidea, Metodo K-Medoides, K=3") +
  theme_bw() +
  theme(legend.position = "bottom")
```

Tras analizar el gráfico, vemos que el solapamiento es similar al método de k-medias, por lo que es menor en comparación con los métodos anteriores y, por tanto, la explicación de los clusters es mejor, puesto que podemos diferenciarlos claramente. Además, la primera componente principal (Dim1) sigue siendo la que más contribuye a la variabilidad de los datos, explicando el 68.8% de la misma, mientras que la segunda componente principal (Dim2) explica un 8.8%.

### Selección y validación de modelos

Aunque nos podemos hacer una idea de que los métodos de partición (k-medias y k-medoides) nos están dando mejores resultados, vamos a comparar el coeficiente de silhoutte en cada cluster para cada método para elegir el método con mayor seguridad.

```{r}
plot(silhouette(grupos1, midist), col=rainbow(2), border=NA, main = "WARD")
plot(silhouette(grupos1b, midist), col=rainbow(3), border=NA, main = "WARD")
plot(silhouette(grupos2, midist), col=rainbow(2), border=NA, main = "AVERAGE")
plot(silhouette(grupos2b, midist), col=rainbow(4), border=NA, main = "AVERAGE")
plot(silhouette(clust3$cluster, midist), col=rainbow(3), border=NA, main = "K-MEDIAS")
plot(silhouette(clust4$clustering, midist), col=rainbow(3), border=NA, main = "K-MEDOIDES")
```

Descartamos el método de Ward y el de la media por el elevado número de observaciones con valores negativos en el estadístico de Silhoutte, es decir, observaciones clasificadas en cluster erróneos. Entre los métodos de partición de k-medias y k-medoides las diferencias son prácticamente inexistentes. Nos decantamos por el método de k-medias porque tiene un valor medio del estadístico de Silhoutte ligeramente superior.

## INTERPRETACIÓN DE LOS RESULTADOS DEL CLUSTERING

Vamos a realizar un PCA para ver qué variables han contribuido más a la determinación de clusters con el algoritmo de k-medias.

```{r}
misclust = factor(clust3$cluster)#Convertimos a factor los clusters de k-medias para ver cómo se distribuye
```

Realizamos el PCA

```{r}
misclust = factor(clust3$cluster)
miPCA = PCA(log_daly, scale.unit = FALSE, graph = FALSE)
eig.val = get_eigenvalue(miPCA)
Vmedia = 100 * (1/nrow(eig.val))
fviz_eig(miPCA, addlabels = TRUE) +
  geom_hline(yintercept=Vmedia, linetype=2, color="red")
```

Graficamos las contribuciones de las variables para las diferentes dimensiones

```{r}
fviz_contrib(miPCA, choice="var", axes=c(1, 2, 3))
```

El gráfico muestra la contribución de diversas variables a las dimensiones 1, 2 y 3 en un análisis de componentes principales (PCA). Las barras representan el porcentaje de contribución de cada variable, con las "Enf_Tropicales" (enfermedades tropicales) siendo la que más contribuye, con más del 20%. La línea roja punteada indica el umbral de contribución promedio esperada (alrededor del 5%), destacando las variables que superan esta contribución promedio. Las variables más significativas después de "Enf_Tropicales" son "Des_Nutricio" (desnutrición) y "Enf_ETS" (enfermedades de transmisión sexual), entre otras. Esto sugiere que estas variables son las más influyentes en las primeras tres dimensiones del análisis.

El hecho de que estas enfermedades o estos problemas sean tan importantes para la creación de las dimensiones, se puede deber a su gran variabilidad y a la diferencia de incidencia entre países, estas suelen tener mayor importancia en países subdesarrollados, produciendo graves problemas sobre estos y una gran cantidad de muertes, y por otra parte en los países más desarrollados se cuenta con protección ante estos, produciendo esa variabilidad.

```{r}
p1 = fviz_pca_ind(miPCA, geom = "point", habillage = misclust, addEllipses = FALSE, 
             palette = rainbow(3))
p2 = fviz_pca_var(miPCA)
grid.arrange(p1, p2, nrow = 1)
```

Tras observar el gráfico de scores, vemos como es posible diferenciar los clusters únicamente con la primera dimesión, ya que aunque se produzcan algunos solapamientos la diferenciación entre ellos es muy buena. Tras obervar el gráfico de las variables, observamos como la primera dimensión está explicada por 'Enf_Tropicales', 'Inf_Entericas','Des_Maternos' y 'Des_Neonatales' entre otras, por lo que la primera dimensión representa enfermedades relacionadas con países subdesarrollados, ya que son producto de infecciones y de falta de alimentación , y por tanto son variables que tienen una gran variabildad entre los diferentes pasies.. Por otra parte, en el caso de la segunda dimensión está explicada por 'ETS' y 'Vio_IntPer'.

```{r}
p1 = fviz_pca_ind(miPCA, geom = "point", habillage = misclust, addEllipses = FALSE, axes = 3:4,
             palette = rainbow(3))
p2 = fviz_pca_var(miPCA, axes = 3:4)
grid.arrange(p1, p2, nrow = 1)
```

Al observar la tercera y cuarta dimensión podemos concluir que es la primera dimensión del PCA la que configura los clusters obtenidos.

```{r}
mediasCluster = aggregate(log_daly, by = list("cluster" = misclust), mean)[,-1]
rownames(mediasCluster) = paste0("c",1:3)
kable(t(round(mediasCluster,2)))
```

```{r}
matplot(t(mediasCluster), type = "l", col = rainbow(3), ylab = "", xlab = "", lwd = 2,
        lty = 1, main = "Perfil medio de los clusters", xaxt = "n")
axis(side = 1, at = 1:ncol(log_daly), labels = colnames(log_daly), las = 2)
legend("topleft", as.character(1:3), col = rainbow(3), lwd = 2, ncol = 3, bty = "n")
```

El gráfico muestra los perfiles medios de tres clusters (agrupaciones) identificados en un análisis de clustering. Cada línea representa un cluster diferente: el cluster 1 está en rojo, el cluster 2 en verde y el cluster 3 en azul. El eje horizontal representa diferentes variables o factores (como "Autolesion," "Conflitos," "Enf_Tropicales," etc.), mientras que el eje vertical representa la media de las puntuaciones normalizadas de estas variables dentro de cada cluster.

El primer grupo toma valores bajos en la mayoría de variables, exceptuando algunas como las autolesiones y el consumo de sustancias. El segundo grupo toma valores cercanos a la medida en todas las variables. El tercer grupo, toma valores elevados en casi todas las variables, destacando las enfermedades tropicales, las infecciones entéricas y las ETS entre otras. Podemos suponer que los grupos están asociados respectivamente a los países desarrollados, en vías de desarrollo y subdesarrollados.

```{r}
log_daly<-cbind(log_daly, cluster=misclust)
```

```{r}
write_xlsx(log_daly, "C:/Users/prado/OneDrive/Escritorio/2º GCD/MDP I/PROYECTO/datos_discriminante.xlsx")
```
